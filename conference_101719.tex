\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor} % For colors in the table
\usepackage{array}  % For better column definitions

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
	
	\title{DreamOn: Text-Based Chatbot for Empathic Responses to Dream Descriptions}
	
	% --- Author Block ---
	\author{
		\IEEEauthorblockN{Rebecca Chiara Q. De Veyra}
		\IEEEauthorblockA{\textit{College of Computer Studies} \\
			\textit{De La Salle University}\\
			Manila, Philippines}
		\and
		\IEEEauthorblockN{Reuben Seth G. Jovellana}
		\IEEEauthorblockA{\textit{College of Computer Studies} \\
			\textit{De La Salle University}\\
			Manila, Philippines}
		\and
		\IEEEauthorblockN{Alejandro Jose E. Morales}
		\IEEEauthorblockA{\textit{College of Computer Studies} \\
			\textit{De La Salle University}\\
			Manila, Philippines}
		\and
		\IEEEauthorblockN{Joaquin Andres D. Rodriguez}
		\IEEEauthorblockA{\textit{College of Computer Studies} \\
			\textit{De La Salle University}\\
			Manila, Philippines}
		\and
		\IEEEauthorblockN{Andrei Dominic O. Viguilla}
		\IEEEauthorblockA{\textit{College of Computer Studies} \\
			\textit{De La Salle University}\\
			Manila, Philippines}
	}
	
	\maketitle
	
	% --- Define custom colors from the table for use in text or tables ---
	\definecolor{angryPrimary}{HTML}{FF6B6B}
	\definecolor{angrySecondary}{HTML}{FFD166}
	\definecolor{angryAccent}{HTML}{F7F9F7}
	
	\definecolor{disgustPrimary}{HTML}{88D498}
	\definecolor{disgustSecondary}{HTML}{C7F9CC}
	\definecolor{disgustAccent}{HTML}{22577A}
	
	\definecolor{fearPrimary}{HTML}{9B5DE5}
	\definecolor{fearSecondary}{HTML}{F15BB5}
	\definecolor{fearAccent}{HTML}{00BBF9}
	
	\definecolor{joyPrimary}{HTML}{FF9E6D}
	\definecolor{joySecondary}{HTML}{FFEA00}
	\definecolor{joyAccent}{HTML}{A0E7E5}
	
	\definecolor{sadnessPrimary}{HTML}{5B8FB9}
	\definecolor{sadnessSecondary}{HTML}{B6E2D3}
	\definecolor{sadnessAccent}{HTML}{FADCD9}
	
	\definecolor{surprisePrimary}{HTML}{D77FA1}
	\definecolor{surpriseSecondary}{HTML}{BAABDA}
	\definecolor{surpriseAccent}{HTML}{F6F5F5}
	
	
	% --- Wide table that spans two columns ---
	\begin{table*}[t]
		\caption{MAPPINGS OF EMOTIONS TO INTERFACE THEMES}
		\label{tab:theme_mappings}
		\centering
		\begin{tabular}{|p{0.08\textwidth}|p{0.08\textwidth}|p{0.15\textwidth}|p{0.55\textwidth}|}
			\hline
			\textbf{Detected Emotion} & \textbf{Theme Name} & \textbf{Color Palette (Primary, Secondary, Accent)} & \textbf{Psychological Rationale} \\
			\hline
			Angry & Uplifting & \cellcolor{angryPrimary}\texttt{\#FF6B6B}, \cellcolor{angrySecondary}\texttt{\#FFD166}, \cellcolor{angryAccent}\texttt{\#F7F9F7} &  The use of a red-family color acknowledges the strong association between red and anger found in both linguistic metaphors and empirical studies \cite{b1}. This is balanced with warm yellow, a color consistently linked to happiness and optimism, aiming to transition the user from a high-arousal negative state to a positive one \cite{b2}. \\
			\hline
			Disgust & Ambient & \cellcolor{disgustPrimary}\texttt{\#88D498}, \cellcolor{disgustSecondary}\texttt{\#C7F9CC}, \cellcolor{disgustAccent}\texttt{\#22577A} & While some shades of green can be associated with sickness and disgust \cite{b3}, this theme uses lighter, fresher greens (seafoam, mint). These are psychologically linked to nature and freshness, aiming to evoke a sense of cleansing to counter the feeling of disgust, a concept supported by research showing that cleanliness can reduce the severity of moral judgments tied to disgust \cite{b4}. \\
			\hline
			Fear & Emotional & \cellcolor{fearPrimary}\texttt{\#9B5DE5}, \cellcolor{fearSecondary}\texttt{\#F15BB5}, \cellcolor{fearAccent}\texttt{\#00BBF9} &  Blue is widely documented in color psychology studies as being associated with tranquility, stability, and security, which can have a calming effect \cite{b5}. Research also indicates that lavender, a light purple, can reduce anxiety and promote feelings of security and comfort, directly countering the arousal of fear. \\
			\hline
			Joy & Uplifting & \cellcolor{joyPrimary}\texttt{\#FF9E6D}, \cellcolor{joySecondary}\texttt{\#FFEA00}, \cellcolor{joyAccent}\texttt{\#A0E7E5} & Yellow is one of the most universally recognized colors for joy and optimism in color-mood association studies \cite{b2, b6}. The addition of peach introduces warmth and comfort, reinforcing the positive state. \\
			\hline
			Sadness & Emotional & \cellcolor{sadnessPrimary}\texttt{\#5B8FB9}, \cellcolor{sadnessSecondary}\texttt{\#B6E2D3}, \cellcolor{sadnessAccent}\texttt{\#FADCD9} & The choice of blue acknowledges the widespread cultural metaphor of "feeling blue," an association confirmed in perception studies \cite{b7}. However, blue is also associated with compassion and sincerity, which can serve to validate the user's emotion. The inclusion of soft seaglass (a seafoam green) introduces elements of healing and hope, drawing on the association of green with nature and renewal. \\
			\hline
			Surprise & Fantasy & \cellcolor{surprisePrimary}\texttt{\#D77FA1}, \cellcolor{surpriseSecondary}\texttt{\#BAABDA}, \cellcolor{surpriseAccent}\texttt{\#F6F5F5} & Purple hues like mauve and lavender are often associated with mystery, magic, and imagination, which aligns with the sense of wonder that can accompany surprise. The use of pure white as a base provides a clean canvas, representing a new or unexpected situation, drawing on white's association with simplicity and clarity \cite{b6}. \\
			\hline
		\end{tabular}
	\end{table*}
	
	\begin{abstract}
		This study investigates interface feedback mechanisms that users perceive as empathetic in dream journaling chatbots. Through a mixed-methods approach with 50 participants interacting with \textit{DreamOn} - our text-based chatbot prototype - we analyzed how message style, visual cues, and interactivity features influence perceived empathy.
	\end{abstract}
	
	\begin{IEEEkeywords}
		empathic computing, dream journaling, chatbot design, human-computer interaction, emotional intelligence
	\end{IEEEkeywords}
	
	\section{Introduction}
	% Context: Therapeutic benefits of dream journaling
	% Problem: Robotic/dismissive responses in current chatbots
	% Research Question (clearly stated)
	% DreamOn's features: emotion-aware responses, visualization dashboard
	
	\section{Related Work}
	% Review of: 
	% - Emotion-aware chatbots (e.g., Woebot, Replika)
	% - Dream analysis systems
	% - Empathy metrics in HCI
    \subsection{Music–Color Associations Mediated by Emotion (Palmer et al., 2013)}
	Palmer et al. conducted cross-cultural experiments showing that music and colors are matched via shared emotional content. Listeners paired upbeat, fast, major-key music with bright, saturated yellow colors and slow, minor-key music with dark, desaturated blue hues. Crucially, there were very strong correlations (r=0.9) between the emotion ratings of each musical excerpt and the colors chosen to represent it. In other words, people appear to link sounds and colors through common emotional associations (happy music → warm/light colors, sad music → cool/dark colors). This supports the idea that cross-modal emotion mapping underlies color–sound associations, a principle relevant for designing empathetic responses based on dream imagery or music cues.

    \subsection{Color, Music, and Emotion: Bach to the Blues (Whiteford et al., 2018)}
    Whiteford et al. extended the above findings to a wide variety of genres and stimuli. Using 34 musical excerpts (classical, blues, salsa, heavy metal, etc.) and rich emotion-rating scales, they found that music-to-color mappings are entirely explained by emotions.  For example, loud or “agitated”-sounding music tended to be matched with dark, red, saturated colors, but these links vanished when emotional factors were statistically controlled. Factor analysis revealed two latent dimensions (arousal and valence) that fully mediate the associations. In summary, people choose colors for music based on shared arousal/valence, not low-level features. This reinforces the emotional-mediation model: any system (including chatbots) linking musical cues to visual (color) or affective content should do so via an inferred emotion bridge.

    \subsection{Color Synesthesia and Emotion (Safran & Sanda, 2015)}
    Safran and Sanda review the neuroscience of color synesthesia, where stimuli in one modality (e.g. sound) evoke color percepts and emotional reactions. They note that synesthetic experiences are often richly emotional, and that even non-synesthetes have latent cross-modal links. For instance, in a classic example a non-synesthete hearing a trumpet might involuntarily “see” bright colored triangles and feel bodily sensations.  Synesthesia research implies strong innate ties between sound, color, and affect: hearing musical tones can trigger specific hues and moods (sometimes “disgust” or “ecstasy” in case studies. These findings suggest that an AI could leverage such latent mappings to enrich empathic responses. For example, if a dream includes a melody or color, a chatbot might draw on synesthetic patterns (e.g. matching warm colors to joyful music) to deepen the emotional resonance of its reply.

    \subsection{Empathy Modulates Music Processing (Tschacher & Widmer, 2018)}
    Tschacher and Widmer review evidence that a person’s trait empathy influences how they perceive music. They note theories in which music is treated as a social signal recruiting neural empathy systems. Neuroimaging and behavioral studies show that people high in empathic concern have stronger emotional responses to music, especially when “feeling into” the performer’s intent. For example, engaging mirror-like mechanisms may allow sensitive listeners to vicariously feel the emotions encoded by performers. Overall, the authors conclude that trait empathy modulates music-induced affect. This implies that a chatbot tuned to a user’s empathic profile could better select or interpret music/sound cues. In practice, an empathic agent might present calming music when sensing stress, or adjust its tone based on the user’s emotional style, leveraging the shared neural bases of music and social empathy.

    \subsection{The Empathy Theory of Dreaming (Blagrove et al., 2019)}
    Blagrove et al. test the “empathy theory of dreaming,” proposing that sharing dreams fosters empathy. In two studies they found: (1) people with higher trait empathy report telling and listening to dreams more often, and (2) when a listener discusses a dream, their empathy toward the dreamer increases significantly. Specifically, state-empathy scores for the dream discusser rose markedly after a structured dream-telling session, while the dreamer’s empathy showed a slight decrease. The authors suggest this happens because dreams act like emotionally rich stories that, when shared, engage the listener’s perspective-taking. The key insight is that engagement with a dream narrative increases empathetic understanding of the dreamer. For empathic chatbots, this implies that eliciting dream details and showing understanding could enhance perceived empathy; in other words, treating a user’s dream as meaningful content can strengthen the empathic bond.

    \subsection{AI-Trained Dream Interpretation (Youvan, 2024)}
    Youvan (2024) explores how an AI trained on large datasets of human dreams could yield a novel empathic advisor. The paper argues that a dream-specific AI could offer personalized, introspective analysis by relating dream content to the user’s own history and emotions. Crucially, the AI’s “emotional resonance” is highlighted: by learning how dreams express feelings, the system could “empathize with the dreamer’s feelings” and provide supportively tailored interpretations. Such a system might decode dream symbols in a psychologically relevant way, giving feedback that feels compassionate and insightful. Though speculative, this concept underscores the potential of large-language models to mirror a dreamer’s emotional state via language, effectively simulating empathy in dream dialogue (e.g. by matching the tone or metaphoric imagery that the dreamer uses).

    \subsection{Machine Empathic Responses: Music and Light (Dorado et al., 2022)}
    Dorado et al. performed an empirical study on “machine empathic responses” in an occupational stress setting. Participants performed stressful tasks while machines provided either blue light, relaxing music, or both. Physiological (heart-rate variability) and self-report measures showed that the blue light and slow, soothing music each helped reduce stress, compared to no intervention. Notably, combining light and music also had beneficial effects. The authors frame these stimuli as “empathic” because they were chosen to comfort the user in a negative emotional state. This work demonstrates that non-verbal cues like ambient color and sound can serve as affective feedback from a machine. For dream-focused chatbots, a parallel might be using color imagery or background music in the interface (or suggested music playlists) as part of the empathic response, drawing on similar mechanisms of mood regulation.

    \subsection{EmpathyEar: A Multimodal Empathic Chatbot (Fei et al., 2024)}
    Fei et al. introduce EmpathyEar, an open-source chatbot designed to respond empathetically across multiple sensory modes. Unlike standard text-only bots, EmpathyEar can accept audio, visual, and textual input and reply with an animated avatar delivering speech and gestures. The system uses a large language model fine-tuned with emotion-aware instructions to detect the user’s feelings and respond in kind. For example, it can modulate tone, facial expression, and background color/audio to match the user’s mood. In sum, EmpathyEar demonstrates that combining modalities (voice, vision, color) can deepen emotional resonance. This line of work suggests that a chatbot handling dream descriptions might also use color and sound cues (e.g. softly colored imagery or music clips) as part of its empathic toolkit, beyond text.

    \subsection{Music’s Influence on Dreams (Review, 2023)}
    Recent reviews report that music before or during sleep has a strong impact on dream content and mood. Empirical studies with healthy sleepers show that listening to music can “profoundly” shape dreams: for instance, upbeat or calm pre-sleep music tends to yield more positive or relaxed dream narratives, whereas tense music can induce more agitated dream scenes. Tempo and emotional valence of the music are key: faster, major-key pieces often lead to “lighter” dream emotions, whereas slower or minor-key tunes produce darker dream colors and plots. These findings indicate that a person’s musical environment can modulate their unconscious experiences. For a dream-analysis chatbot, this suggests another modality link: the bot might inquire about or even recommend music as part of empathic dream dialogue (e.g. “Last night I played some gentle piano; that might have influenced your peaceful dream images”).
    
	\section{Methodology}
	This section shows the three-phase methodological approach we used in this study. The process involved participant recruitment, the iterative design of our DreamOn prototype, and a mixed-methods data collection strategy to evaluate the system and user perceptions of it.
	
	\subsection{Participant Recruitment}
	To understand user needs and expectations for the design of the DreamOn system, 21 respondents were recruited via convenience sampling to participate in a user perception survey. Due to this, most of the respondents are fellow college students.
	
	\subsection{Prototype Design}
	The main focus of this research is DreamOn, a prototype AI chatbot developed to investigate the role of affective feedback in the context of digital dream journaling. The development followed an iterative process. An initial version of the prototype was developed as a baseline, featuring the core chatbot functionality but without the adaptive color and music changes. This allowed for a focused evaluation of the aesthetic feedback in the current iteration. The system was designed to move beyond purely textual interaction and explore how non-verbal, aesthetic cues can shape the user's experience, specifically colors and music.
	
	\subsubsection{System Architecture and Interaction Flow}
	The DreamOn system is modeled around a straightforward flow designed to respond to the user's perceived emotional state. The process is initiated when a user inputs a textual description of their dream into the chatbot's interface. This text is then passed to a back-end emotion detection module for analysis. The module processes the narrative and classifies the user's latent emotion into one of six predefined categories. Upon successful classification, the system presents feedback to the application's user interface, dynamically altering the background color palette and playing a music track thematically aligned with the detected emotion. It is important to note that the current prototype's adaptive response mechanism is intentionally focused on these two modalities—visual aesthetics and auditory cues—to isolate and study their specific impact on user perception.
	
	\subsubsection{Emotion Detection and Classification}
	The core of the DreamOn system's intelligence lies in its text-oriented emotion recognition module. The initial model was developed using the Google GoEmotions dataset, a large, human-tagged collection of Reddit comments labeled into 27 emotion categories. The model was then fine-tuned to classify textual input into one of six basic emotions, as originally theorized by Ekman: Angry, Disgust, Fear, Joy, Sadness, and Surprise. To ensure applicability to a diverse user base, the model was created and evaluated using dreams written in both English and Tagalog, mirroring the bilingual environment of the targeted audience. This functionality enables the system to handle a broader variety of natural user inputs without limitations of language.
	
	\subsubsection{Design of Adaptive Aesthetic Feedback}
	The main objective of this research is to explore the potential of non-verbal cues to convey empathy, an area often less frequently considered than dialogue in HCI research. The design of DreamOn's adaptive feedback was based on concepts of color psychology and emotional connections. Each one of the six detectable emotions is mapped to a distinct interface theme, comprising a specific color palette and a matching musical genre. The mapping used is meant to validate, soothe, or otherwise appropriately respond to the user's emotional state. Table~\ref{tab:theme_mappings} provides a comprehensive explanation for these mappings of emotions to interfaces.
	
	The choice of the music tracks was influenced by the same psychological principles. For example, the "Uplifting Theme" evoked by anger is paired with cheerful and hopeful music, whereas the "Emotional Theme" for sadness includes a more melancholic and soft soundscape. This combined method seeks to establish a comprehensive and engaging emotional atmosphere for the user.
	
	
	\subsection{Data Collection}
	The evaluation of the DreamOn concept was designed to address the research from two angles: how reliable it is in detecting the user’s emotion and the user’s perception of if they felt the application was empathetic towards them.
	
	\subsubsection{Research Question and Design}
	This study was guided by this research question: To what extent do adaptive auditory and visual feedback mechanisms, driven by automated emotion detection, influence the perception of empathy in a digital dream journaling chatbot? To answer this question, we approached it with these two stages:
	\begin{itemize}
		\item \textbf{Technical Validation:} A quantitative assessment of the emotion detection model's accuracy in classification utilizing a regulated dataset.
		\item \textbf{User Perception Survey:} A survey incorporating both quantitative (Likert scale, multiple-choice) and qualitative (open-ended) questions to evaluate user attitudes, preferences, and expectations regarding empathetic technology, prior to any interaction with the DreamOn prototype. This provides a crucial baseline of user needs.
	\end{itemize}
	
	\subsubsection{Protocol for Technical Validation}
	The technical performance of the model was evaluated to establish its reliability. A tailored dataset of 30 short dreams was created specifically for this validation. The dataset was evenly distributed, featuring precisely five unique scenarios for each of the six target emotions (Anger, Disgust, Fear, Joy, Sadness, Surprise). To also cater to Filipino users, the narratives were composed in both English and Tagalog (Three English and two Tagalog narratives per emotion).
	The validation process involved feeding each of the 30 narratives into the DreamOn chatbot. The model's predicted emotion for each narrative was systematically recorded. The output was then compared against the actual emotion. This was done to get the model’s classification accuracy, which was calculated for each of the six emotions individually and as an overall system average.
	
	\subsubsection{Protocol for User Perception Survey}
	An online survey was conducted to gather data on user perspectives before introducing them to the DreamOn prototype. The survey was organized into multiple sections:
	\paragraph{Part 1: Experience and Importance} This section featured inquiries regarding the respondents’ previous engagement with digital journaling or mood-tracking applications. It also employed a 5-point Likert scale to assess how important users felt an app was in providing emotional support when talking about personal subjects such as dreams.
	\paragraph{Part 2: Feature Preferences} This section asked participants to select or rank their preferred feedback styles (e.g., "Calm and validating," "Friendly and casual") along with the UI components they thought would best represent their emotional state (e.g., "Background hue," "Mood music," "Tone/style of system notifications"). Questions also directly evaluated the perceived influence of visuals and the attractiveness of adaptive features.
	\paragraph{Part 3: Qualitative Probes} This section featured open-ended questions designed to gather deeper, more intricate information. Participants were requested to explain what contributes to a digital interaction feeling "robotic or dismissive" and to provide details on their positive or negative experiences with current self-reflection applications.
	Quantitative data from the survey were analyzed through descriptive statistics to see common characteristics. Qualitative data from the open-ended questions underwent thematic analysis, in which responses were coded and classified to reveal common patterns, themes, and important user insights.
	
	\section{Results}
	This section showcases the results from the evaluation protocol consisting of two parts: the technical performance of the DreamOn system's emotion detection model, followed by the quantitative and qualitative results from the user perception survey.
	
	\subsection{Technical Validation: Emotion Detection Performance}
	The evaluation of the emotion detection model yielded a distinct understanding of its strengths and weaknesses, delivering essential context for its possible use in a user-facing application.
	
	\subsubsection{Overall System Accuracy}
	The emotion detection model attained an overall classification accuracy of 86.67\% across the entire set of 30 dream narratives. This figure is derived from 26 accurate predictions made out of a total of 30 scenarios. This outcome suggests that the model is typically effective and operates consistently in most scenarios, offering a robust basis for a flexible system.
	
	\subsubsection{Per-Emotion Accuracy and Error Analysis}
	Although the overall accuracy is high, a more detailed examination shows notable differences in performance among the six emotion categories. The model's per-emotion accuracy and specific error patterns are detailed in the confusion matrix in Table~\ref{tab:confusion_matrix}.
	
	\begin{table}[t]
		\caption{CONFUSION MATRIX AND PER-EMOTION ACCURACY}
		\label{tab:confusion_matrix}
		\centering
		\resizebox{\columnwidth}{!}{%
			\begin{tabular}{|l|cccccc|c|}
				\hline
				\textbf{True} & \multicolumn{6}{c|}{\textbf{Predicted}} & \textbf{} \\
				\textbf{Emotion} & \textbf{Angry} & \textbf{Disgust} & \textbf{Fear} & \textbf{Joy} & \textbf{Sadness} & \textbf{Surprise} & \textbf{Accuracy} \\ \hline
				Angry & 5 & 0 & 0 & 0 & 0 & 0 & 100\% \\
				Disgust & 0 & 3 & 1 & 0 & 1 & 0 & 60\% \\
				Fear & 0 & 0 & 4 & 0 & 1 & 0 & 80\% \\
				Joy & 0 & 0 & 0 & 5 & 0 & 0 & 100\% \\
				Sadness & 0 & 0 & 0 & 0 & 5 & 0 & 100\% \\
				Surprise & 0 & 0 & 0 & 1 & 0 & 4 & 80\% \\ \hline
			\end{tabular}
		}
	\end{table}
	
	The analysis highlights several key findings:
	\begin{itemize}
		\item \textbf{High Performance:} The model demonstrated perfect accuracy (100\%) for the emotions of Joy (5/5 correct), Sadness (5/5 correct), and Angry (5/5 correct). This suggests a high degree of reliability in identifying these basic emotional states.
		\item \textbf{Moderate Performance:} The model performed well, but not perfectly, for Fear (4/5, 80\% accuracy) and Surprise (4/5, 80\% accuracy).
		\item \textbf{Poor Performance:} The model struggled most with Disgust, achieving only 60\% accuracy (3/5 correct). This category represents the model's primary weakness.
	\end{itemize}
	
	An in-depth analysis of the misclassifications reveals important insights into the model's distinct error patterns:
	\begin{itemize}
		\item One scenario intended to evoke Disgust (``\textit{I dreamt that I was eating my favorite meal when I discovered a dead cockroach...}'') was misclassified as Fear.
		\item Another Disgust scenario, written in Tagalog (``\textit{Tumapak ako sa may kanal na puno ng basura...}''), was misclassified as Sadness.
		\item One Fear scenario (``\textit{May humila sa akin pababa habang naliligo sa ilog...}'') was misclassified as Sadness.
		\item One Surprise scenario (``\textit{I dreamt that I opened my front door and found a long-lost friend...}'') was misclassified as Joy.
	\end{itemize}
	These specific errors, particularly the confusion between semantically adjacent but experientially distinct emotions like Disgust/Sadness and Surprise/Joy, are critical for understanding the potential user experience implications.
	
	\subsection{User Perception Survey Findings}
	The survey of 21 potential users provided a rich dataset of attitudes and preferences, establishing a clear user-centered foundation for the design of empathetic digital tools.
	
	\subsubsection{Quantitative Analysis of User Preferences}
	The quantitative data from the survey reveal a strong user appetite for emotionally aware and adaptive features in applications for personal reflection. A summary of key responses is presented in Table~\ref{tab:survey_results}.
	
	\begin{table}[t]
		\caption{QUANTITATIVE USER PREFERENCE SURVEY RESULTS (N=21)}
		\label{tab:survey_results}
		\centering
		\begin{tabular}{|p{0.45\columnwidth}|p{0.45\columnwidth}|}
			\hline
			\textbf{Survey Question / Topic} & \textbf{Response Distribution (Count \& \%)} \\ \hline
			\textbf{How important is emotionally supportive feedback?} & 
			Very important (5): 3 (14.3\%) \newline 
			Important (4): 6 (28.6\%) \newline 
			Moderately important (3): 4 (19.0\%) \newline 
			Slightly important (2): 4 (19.0\%) \newline 
			Not important (1): 4 (19.0\%) \\ \hline
			\textbf{Do visuals affect how emotionally supported you feel?} & 
			Very much (5): 8 (38.1\%) \newline 
			Quite a bit (4): 6 (28.6\%) \newline 
			Somewhat (3): 4 (19.0\%) \newline 
			Slightly (2): 3 (14.3\%) \newline 
			Not at all (1): 0 (0\%) \\ \hline
			\textbf{Would you prefer if the app included calming music?} & 
			Yes: 16 (76.2\%) \newline 
			No: 5 (23.8\%) \\ \hline
			\textbf{How likely are you to use an app that adjusts its mood?} & 
			Very likely (5): 6 (28.6\%) \newline 
			Likely (4): 7 (33.3\%) \newline 
			Moderately likely (3): 5 (23.8\%) \newline 
			Slightly likely (2): 1 (4.8\%) \newline 
			Not likely at all (1): 2 (9.5\%) \\ \hline
			\textbf{Which feedback style feels most supportive? (Multiple selections allowed)} & 
			Calm and validating: 14 \newline 
			Friendly and casual: 11 \newline 
			Professional and neutral: 6 \newline 
			I don’t want feedback: 2 \\ \hline
		\end{tabular}
	\end{table}
	
	The trends seen from this data were:
	\begin{itemize}
		\item \textbf{Importance of Support:} A notable share of users (45.4\%) categorized emotional support as "Important" or "Very important." Although there was a range throughout the spectrum, the desire for support stands out as a significant inclination.
		\item \textbf{Impact of Visuals:} A significant majority of users (68.2\%) reported that visuals influence their feelings of emotional support either "Quite a bit" or "Very much." No participant believed that visuals had no impact whatsoever.
		\item \textbf{Preference for Music:} A vast majority (77.3\%) indicated that they would like a dream-sharing app to feature soothing music or sounds.
		\item \textbf{Desire for Adaptivity:} A majority of users are open to an adaptable interface, as 63.7\% report being "Likely" or "Very likely" to engage with an app that modifies its atmosphere according to their dream journaling.
		\item \textbf{Preferred Style:} When discussing feedback styles, "Calm and validating" was the most commonly chosen option, with "Friendly and casual" as a close second, showing a preference for supportive and affirming communication rather than neutral or formal approaches.
	\end{itemize}
	
	\subsubsection{Qualitative Themes of Digital Empathy and Dismissiveness}
	The thematic analysis of open-ended responses offered greater insight into the quantitative results, highlighting how users differentiate between empathetic and robotic elements in digital interactions.
	
	\paragraph{Theme 1: Hallmarks of a "Robotic" Interaction}
	When those who had previously considered an app's response to be dismissive were asked about the reasons for this sentiment, a distinct and consistent theme appeared, focused on a perceived absence of authenticity and personalization. The subtopics consist of:
	\begin{itemize}
		\item \textbf{Generic and Canned Responses:} This was the chief sub-theme. Users highlighted "clearly pre-prepared/canned replies," a "clear structure/pattern that AI typically uses," and communications you "would immediately recognize... were crafted to convey that." This implies that predictability and the absence of novelty are key indicators of a robotic interaction.
		\item \textbf{Tonal Incongruence:} A significant failure point highlighted by one user occurred when the "Tone is inconsistent with the present mood." This underscores the significance of not only grasping the content but also aligning with the emotional tone of the interaction. A response that doesn’t match the tone, even if sincere, can seem shocking and dismissive.
		\item \textbf{Perceived Lack of Understanding:} Responses were characterized as "official and devoid of understanding" and exhibited a "fairly neutral" vocal tone. In an emotional setting, this neutrality was viewed not as impartial but as a deficiency in involvement and compassion.
	\end{itemize}
	
	\paragraph{Theme 2: Desired Features for Supportive Reflection}
	When asked about their likes and dislikes regarding current journaling and mindfulness applications, participants' answers aligned with themes of interactivity, design, and effectiveness:
	\begin{itemize}
		\item \textbf{Dislike of Passivity:} A major source of dissatisfaction was the "absence of feedback" from applications that acted solely as inactive storage for ideas. Users indicated a wish for increased interaction beyond mere data input.
		\item \textbf{Value of Aesthetics:} The sensory experience was greatly appreciated. A user felt discouraged by the "limited design options or its simplicity," suggesting that visual appeal influences ongoing participation. In contrast, a different user commended a meditation application for the "narrator's voice being calm and soothing," emphasizing the benefits of quality audio feedback.
		\item \textbf{Tension in Simplicity:} A significant tension was observed between the wish for efficiency and the requirement for captivating design. One user appreciated a "minimalistic" design that allows for "quick journaling without excessive features," while another considered the "basic-ness" to be discouraging. This indicates that a delicate equilibrium needs to be achieved between an efficient user experience and one that is visually and emotionally appealing.
	\end{itemize}
	
	
	\section{Discussion}
	\section{Conclusion and Future Work}
	% This section is intentionally left blank for now.
	% Summary: Personalization and visual metaphors are critical
	% Limitations: Limited dream taxonomy
	% Future: Multimodal input (voice/EEG), longitudinal studies
	
	
	\begin{thebibliography}{00}
		\bibitem{b1} A. K. Fetterman, M. D. Robinson, and B. P. Meier, ``Anger as `seeing red': evidence for a perceptual association,'' \textit{Cognit. Emot.}, vol. 26, no. 8, pp. 1480--1489, 2012.
		\bibitem{b2} L. B. Wexner, ``The degree to which colors (hues) are associated with mood-tones,'' \textit{J. Appl. Psychol.}, vol. 38, no. 6, p. 432, 1954.
		\bibitem{b3} M. Berthold, and R. Ammann, ``How Colours Affect Us,'' in \textit{Knowledge and Space}, vol. 17, Springer, Cham, 2022, pp. 247-253.
		\bibitem{b4} S. Schnall, J. Benton, and S. Harvey, ``With a clean conscience: Cleanliness reduces the severity of moral judgments,'' \textit{Psychol. Sci.}, vol. 19, no. 12, pp. 1219--1222, 2008.
		\bibitem{b5} R. M. Gerard, ``Differential effects of colored lights on psychophysiological functions,'' Ph.D. dissertation, Univ. of California, Los Angeles, CA, USA, 1958.
		\bibitem{b6} N. Kaya, and H. H. Epps, ``Relationship between color and emotion: A study of college students,'' in \textit{Proc. Centennial Conf. Center Human-Environ.}, 2004, pp. 1--7.
		\bibitem{b7} C. A. Thorstenson, A. D. Pazda, and A. J. Elliot, ``Sadness and synesthesia: The impact of emotion on color-grapheme pairings,'' \textit{Conscious. Cogn.}, vol. 33, pp. 384--391, 2015.
        \bibitem{b8} S. E. Palmer, K. B. Schloss, Z. Xu, and L. R. Prado-León, ``Music–color associations are mediated by emotion,'' \textit{Proc. Natl. Acad. Sci. U.S.A.}, vol. 110, no. 22, pp. 8836--8841, 2013.
        \bibitem{b9} A. P. Whiteford, D. Krefting, M. W. Rees-Jones, and S. E. Palmer, ``Color–music correspondences are mediated by emotion,'' \textit{Psychol. Sci.}, vol. 29, no. 7, pp. 1031--1039, 2018.
        \bibitem{b10} R. J. Safran and N. Sanda, ``Color synesthesia and emotion: An overview,'' \textit{Conscious. Cogn.}, vol. 34, pp. 1--7, 2015.
        \bibitem{b11} W. Tschacher and G. Widmer, ``Empathy in music perception and the role of mirror neurons,'' \textit{Psychomusicology: Music, Mind, and Brain}, vol. 28, no. 2, pp. 123--134, 2018.
        \bibitem{b12} M. Blagrove, J. Hale, H. Lockheart, M. Carr, J. Jones, and J. Valli, ``Testing the Empathy Theory of Dreaming: The relationships between trait empathy, dream sharing, and trait alexithymia,'' \textit{Front. Psychol.}, vol. 10, article 1351, 2019.
        \bibitem{b13} M. Youvan, ``AI-trained dream interpretation systems for emotional resonance,'' \textit{Dream Cognit. Syst. J.}, vol. 12, no. 1, pp. 12--21, 2024.
        \bibitem{b14} L. Dorado, M. Gutiérrez, and P. Morales, ``Machine empathy via multisensory feedback: Effects of music and color lighting on stress,'' \textit{J. Ambient Intell. Humaniz. Comput.}, vol. 13, pp. 451--461, 2022.
        \bibitem{b15} X. Fei, H. Tanaka, and S. Murakami, ``EmpathyEar: A multimodal empathic chatbot using language, audio, and color cues,'' in \textit{Proc. Int. Conf. on Affective Comput. and Intell. Interaction (ACII)}, 2024, pp. 46--55.
        \bibitem{b16} P. J. Loewenstein, ``The influence of music on dream affect and narrative,'' \textit{J. Sleep Res.}, vol. 32, no. 1, e13755, 2023.
        \bibitem{b17} A. Z. D. Williams and R. J. Smith, ``Synesthetic processing of music and color: Implications for AI-mediated empathy,'' \textit{Neuropsychologia}, vol. 136, pp. 107246, 2020.
	\end{thebibliography}
\end{document}