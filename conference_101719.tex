\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor} % For colors in the table
\usepackage{array}  % For better column definitions

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
	
	\title{DreamOn: Text-Based Chatbot for Empathic Responses to Dream Descriptions}
	
	% --- Author Block ---
	\author{
		\IEEEauthorblockN{Rebecca Chiara Q. De Veyra}
		\IEEEauthorblockA{\textit{College of Computer Studies} \\
			\textit{De La Salle University}\\
			Manila, Philippines}
		\and
		\IEEEauthorblockN{Reuben Seth G. Jovellana}
		\IEEEauthorblockA{\textit{College of Computer Studies} \\
			\textit{De La Salle University}\\
			Manila, Philippines}
		\and
		\IEEEauthorblockN{Alejandro Jose E. Morales}
		\IEEEauthorblockA{\textit{College of Computer Studies} \\
			\textit{De La Salle University}\\
			Manila, Philippines}
		\and
		\IEEEauthorblockN{Joaquin Andres D. Rodriguez}
		\IEEEauthorblockA{\textit{College of Computer Studies} \\
			\textit{De La Salle University}\\
			Manila, Philippines}
		\and
		\IEEEauthorblockN{Andrei Dominic O. Viguilla}
		\IEEEauthorblockA{\textit{College of Computer Studies} \\
			\textit{De La Salle University}\\
			Manila, Philippines}
	}
	
	\maketitle
	
	% --- Define custom colors from the table for use in text or tables ---
	\definecolor{angryPrimary}{HTML}{FF6B6B}
	\definecolor{angrySecondary}{HTML}{FFD166}
	\definecolor{angryAccent}{HTML}{F7F9F7}
	
	\definecolor{disgustPrimary}{HTML}{88D498}
	\definecolor{disgustSecondary}{HTML}{C7F9CC}
	\definecolor{disgustAccent}{HTML}{22577A}
	
	\definecolor{fearPrimary}{HTML}{9B5DE5}
	\definecolor{fearSecondary}{HTML}{F15BB5}
	\definecolor{fearAccent}{HTML}{00BBF9}
	
	\definecolor{joyPrimary}{HTML}{FF9E6D}
	\definecolor{joySecondary}{HTML}{FFEA00}
	\definecolor{joyAccent}{HTML}{A0E7E5}
	
	\definecolor{sadnessPrimary}{HTML}{5B8FB9}
	\definecolor{sadnessSecondary}{HTML}{B6E2D3}
	\definecolor{sadnessAccent}{HTML}{FADCD9}
	
	\definecolor{surprisePrimary}{HTML}{D77FA1}
	\definecolor{surpriseSecondary}{HTML}{BAABDA}
	\definecolor{surpriseAccent}{HTML}{F6F5F5}
	
	
	% --- Wide table that spans two columns ---
	\begin{table*}[t]
		\caption{MAPPINGS OF EMOTIONS TO INTERFACE THEMES}
		\label{tab:theme_mappings}
		\centering
		\begin{tabular}{|p{0.08\textwidth}|p{0.08\textwidth}|p{0.15\textwidth}|p{0.55\textwidth}|}
			\hline
			\textbf{Detected Emotion} & \textbf{Theme Name} & \textbf{Color Palette (Primary, Secondary, Accent)} & \textbf{Psychological Rationale} \\
			\hline
			Angry & Uplifting & \cellcolor{angryPrimary}\texttt{\#FF6B6B}, \cellcolor{angrySecondary}\texttt{\#FFD166}, \cellcolor{angryAccent}\texttt{\#F7F9F7} &  The use of a red-family color acknowledges the strong association between red and anger found in both linguistic metaphors and empirical studies \cite{b1}. This is balanced with warm yellow, a color consistently linked to happiness and optimism, aiming to transition the user from a high-arousal negative state to a positive one \cite{b2}. \\
			\hline
			Disgust & Ambient & \cellcolor{disgustPrimary}\texttt{\#88D498}, \cellcolor{disgustSecondary}\texttt{\#C7F9CC}, \cellcolor{disgustAccent}\texttt{\#22577A} & While some shades of green can be associated with sickness and disgust \cite{b3}, this theme uses lighter, fresher greens (seafoam, mint). These are psychologically linked to nature and freshness, aiming to evoke a sense of cleansing to counter the feeling of disgust, a concept supported by research showing that cleanliness can reduce the severity of moral judgments tied to disgust \cite{b4}. \\
			\hline
			Fear & Emotional & \cellcolor{fearPrimary}\texttt{\#9B5DE5}, \cellcolor{fearSecondary}\texttt{\#F15BB5}, \cellcolor{fearAccent}\texttt{\#00BBF9} &  Blue is widely documented in color psychology studies as being associated with tranquility, stability, and security, which can have a calming effect \cite{b5}. Research also indicates that lavender, a light purple, can reduce anxiety and promote feelings of security and comfort, directly countering the arousal of fear. \\
			\hline
			Joy & Uplifting & \cellcolor{joyPrimary}\texttt{\#FF9E6D}, \cellcolor{joySecondary}\texttt{\#FFEA00}, \cellcolor{joyAccent}\texttt{\#A0E7E5} & Yellow is one of the most universally recognized colors for joy and optimism in color-mood association studies \cite{b2, b6}. The addition of peach introduces warmth and comfort, reinforcing the positive state. \\
			\hline
			Sadness & Emotional & \cellcolor{sadnessPrimary}\texttt{\#5B8FB9}, \cellcolor{sadnessSecondary}\texttt{\#B6E2D3}, \cellcolor{sadnessAccent}\texttt{\#FADCD9} & The choice of blue acknowledges the widespread cultural metaphor of "feeling blue," an association confirmed in perception studies \cite{b7}. However, blue is also associated with compassion and sincerity, which can serve to validate the user's emotion. The inclusion of soft seaglass (a seafoam green) introduces elements of healing and hope, drawing on the association of green with nature and renewal. \\
			\hline
			Surprise & Fantasy & \cellcolor{surprisePrimary}\texttt{\#D77FA1}, \cellcolor{surpriseSecondary}\texttt{\#BAABDA}, \cellcolor{surpriseAccent}\texttt{\#F6F5F5} & Purple hues like mauve and lavender are often associated with mystery, magic, and imagination, which aligns with the sense of wonder that can accompany surprise. The use of pure white as a base provides a clean canvas, representing a new or unexpected situation, drawing on white's association with simplicity and clarity \cite{b6}. \\
			\hline
		\end{tabular}
	\end{table*}
	
	\begin{abstract}
		Digital journaling applications often lack the capacity to provide emotionally supportive responses, particularly in contexts where users share personal and vulnerable content such as dreams. This study attempts to address this by proposing DreamOn, a text-based chatbot designed to deliver empathic feedback during dream journaling. With this, we investigated interface feedback mechanisms that users perceive as empathetic and analyzed how message style, visual cues, and interactivity features influence perceived empathy. The system analyzes user-inputted dream narratives using an emotion classification model and dynamically adapts its interface through music and color themes based on the detected emotional state. A user perception survey involving 21 participants was conducted to perform needfinding and determine what users would like to see in the app, assessing preferences with audio and visual details, overall design, feedback style, adaptivity, and more, which were then evaluated and implemented accordingly. To evaluate DreamOn’s effectiveness, a technical validation was conducted using 30 bilingual (English and Tagalog) dream samples, which achieved an overall classification accuracy of 86.67\%, with high reliability for core emotions such as anger, joy, and sadness. This study contributes to the field of affective computing by demonstrating how emotion-driven feedback can enhance perceived empathy in chatbot interactions. 

	\end{abstract}
	
	\begin{IEEEkeywords}
		empathic computing, dream journaling, chatbot design, human-computer interaction, emotional intelligence
	\end{IEEEkeywords}
	
	\section{Introduction}
	% Context: Therapeutic benefits of dream journaling
	% Problem: Robotic/dismissive responses in current chatbots
	% Research Question (clearly stated)
	% DreamOn's features: emotion-aware responses, visualization dashboard
 	In recent years, digital platforms have increasingly supported personal reflection and mental health practices, from mood-tracking apps to guided journaling tools. Yet, many of these systems still fall short in offering emotionally responsive interactions. As artificial intelligence becomes more integrated into daily life, there is a growing expectation for systems to not only process information but also respond with empathy. Empathy in computing has become central to the design of human-centered technologies that aim to build trust, comfort, and emotional support. However, despite advancements in conversational AI, most chatbot systems still rely on fixed, text-based replies that often feel robotic or emotionally disconnected. This is especially problematic in contexts like dream journaling, where users may disclose deeply personal experiences and emotional states. Without adequate affective feedback, such interactions risk feeling impersonal or even dismissive, limiting the perceived usefulness of the system.

	This project addresses the challenge by designing a chatbot that can convey empathy not just through language, but through multimodal feedback mechanisms. Our objective is to explore how emotional resonance can be simulated in digital dream-sharing by using adaptive interface cues, such as color themes and music tracks, that align with a user's inferred emotional state. It aims to address the question of \textit{To what extent do adaptive auditory and visual feedback mechanisms, driven by automated emotion detection, influence the perception of empathy in a digital dream journaling chatbot?}
 
	To meet this objective, we developed DreamOn, a text-based chatbot that analyzes user-inputted dream narratives and responds with customized visual and auditory feedback. The system integrates an emotion detection model fine-tuned on multilingual dream data and maps detected emotions to carefully designed color palettes and music themes grounded in psychological theory. By doing so, DreamOn aims to create a more emotionally supportive journaling experience and investigate the role of aesthetic feedback in shaping users’ perceptions of empathy.


	
	\section{Related Work}
	% Review of: 
	% - Emotion-aware chatbots (e.g., Woebot, Replika)
	% - Dream analysis systems
	% - Empathy metrics in HCI
    \subsection{Emotion–Mediated Music–Color Associations}
	Early studies establish that people consistently match musical tones to colors via shared emotions. For example, Palmer et al. (2013) found nearly perfect cross-cultural correlations (r≈0.9) between the emotional character of a musical excerpt and the color people chose to represent it. Major-key, fast (“happy”) music yielded light saturated yellows, while slow, minor-key (“sad”) music elicited dark bluish hues. Whiteford et al. (2018) generalized this result across 34 genres, showing that any apparent perceptual links vanish once emotion (valence/arousal) is controlled: music–color mappings were fully mediated by two latent affect factors. In other words, people choose colors and sounds by common emotional meaning, not by raw acoustic or visual features. Likewise, research on synesthesia suggests even non-synesthetes have latent sound–color–emotion links. Safran and Sanda (2015) review how synesthetic experiences commonly trigger strong emotional reactions, implying robust innate ties between modalities.

    DreamOn builds directly on this emotion-mediation model. We adopt color–sound pairings that mirror these findings (e.g. pairing soothing music with “calming” blues or uplifting melodies with warm yellows) so that the bot’s aesthetic feedback validates and modulates the user’s mood. In contrast to past work which examined human judgments in isolation, DreamOn operationalizes the shared valence/arousal mapping in an interactive interface: its automated emotion detector drives adaptive themes (Table 1) that align music and UI color in the user’s dream-dialogue. Thus, DreamOn extends the music–color research into a real-time empathic HCI setting by leveraging the same cross-modal emotional bridge.
    
    \subsection{Empathy in Music Perception}
    Research also shows that an individual’s empathy trait shapes how they process music. Tschacher and Widmer (2018) review evidence that people high in empathic concern feel music more intensely and “vicariously” engage with the performer’s emotions. For example, studies find greater trait empathy predicts stronger affective responses to expressive music (via mirror‑like neural mechanisms). In other words, empathy is not just a social skill but a “tuning knob” on musical affective impact: those with greater empathy experience music as a more potent social/emotional stimulus.

    DreamOn is informed by this insight. We assume that an empathically tuned chatbot should tailor its musical feedback to the user’s emotional style. Just as empathic listeners prefer music that resonates with their mood, DreamOn selects calming or uplifting tracks based on the detected dream affect. In contrast to prior work, which focused on human differences, DreamOn applies the same principle algorithmically: by diagnosing the dream’s emotion, the bot can “empathize” with the user through genre and tempo, for example choosing soft piano for sadness or upbeat pop for joy. This leverages the shared neural basis of music and social empathy outlined by Tschacher \& Widmer, embedding it into the chatbot’s response logic.

    \subsection{Dream Sharing and Emotional Resonance}
    Blagrove et al. (2019) propose the Empathy Theory of Dreaming, demonstrating that sharing and discussing dreams elicits empathy between speaker and listener. In their studies, people with higher trait empathy tended to tell and listen to more dreams, and structured dream-sharing sessions significantly raised the listener’s state-empathy toward the dreamer. The authors argue that dreams function like emotionally rich stories that engage perspective-taking: listening to a vivid dream narrative boosts empathy for the storyteller. This suggests that treating a dream as meaningful content and asking open-ended questions can deepen understanding.

    Importantly, reviews also note that ambient music can influence dream content and mood (e.g. upbeat pre-sleep music yields more positive dreams, whereas tense music produces more agitated dream scenarios). While not yet widely applied in chatbots, this finding implies that asking about or integrating the user’s musical context could enrich interpretation. DreamOn builds on these insights by encouraging users to describe their dreams fully, then reflecting emotions back through color and sound. By combining narrative engagement (like Blagrove’s empathic dream listening) with emotional validation, DreamOn aims to replicate the bond of dream-sharing. In short, DreamOn treats the user’s dream as an emotional story worth deep listening, aligning with the empathy gains observed in dream-sharing research.

    \subsection{AI-Driven Dream Interpretation}
    Recent work envisions explicit AI systems for dream analysis. Youvan (2024) imagines training an AI solely on large collections of human dreams to deliver personalized, emotionally resonant feedback. His framework emphasizes emotional resonance: such a dream-AI would learn patterns in dream symbols and affect so it can relate content to the dreamer’s history and feelings. By processing dream narratives with contextual understanding, the AI could offer interpretations that acknowledge and validate the dreamer’s emotions. Youvan suggests this system would “simulate empathy” by matching tone and imagery to the user’s affect, thus yielding insights that feel supportive and compassionate.

    DreamOn realizes an early instance of this vision. Unlike prior proposals that are largely conceptual, our chatbot actually combines automated dream-emotion detection with feedback. We leverage large-language models (GoEmotions-tuned) to classify the dream narrative into core emotions and then generate responses. This approach echoes Youvan’s idea of a dream-informed AI, but adds a unique multimodal twist: DreamOn goes beyond text, adapting the interface’s colors and music to mirror the dream’s mood. In doing so, DreamOn not only analyzes dream content linguistically but also materializes empathy through sensory cues. This synthesis of AI dream interpretation with affective UI feedback distinguishes our work from purely text-based dream bots.

    \subsection{Multimodal Empathic Interfaces}
    A growing body of work explores how machines can use multiple modalities to convey empathy. Dorado et al. (2022) experimentally demonstrated that “machine empathic responses” such as gentle blue ambient lighting or relaxing music can physiologically reduce stress. In their study, participants performing stressful tasks experienced lower heart-rate variability (i.e. reduced stress) when exposed to calming blue light or slow music, relative to no intervention. Likewise, Fei et al. (2024) introduced EmpathyEar, a multimodal chatbot that integrates text, audio, and visual signals into empathy. EmpathyEar uses a fine-tuned LLM plus an animated avatar to respond with synchronized speech, facial expressions, and even background color or audio changes. They show that combining modalities (voice, vision, color cues) deepens emotional resonance beyond what text alone can achieve. 
    
    DreamOn follows this multimodal trend, though in a simpler form. Like EmpathyEar, it recognizes the power of color and sound as affective channels. like Dorado’s “machine empathy,” it uses those nonverbal cues to comfort the user. However, unlike many systems (which use humanoid avatars or VR), DreamOn remains a text-based chatbot that subtly shifts its interface theme and playlist according to emotion. This strikes a novel balance: we enrich the basic text dialogue with ambient feedback, rather than overhaul the interaction with full multimodal input/output. In summary, DreamOn builds on prior multimodal empathy research by applying the same principles (ambient color and music as emotional mirrors) specifically in the context of dream journaling.

	\section{Methodology}
	This section shows the three-phase methodological approach we used in this study. The process involved participant recruitment, the iterative design of our DreamOn prototype, and a mixed-methods data collection strategy to evaluate the system and user perceptions of it.
	
	\subsection{Participant Recruitment}
	To understand user needs and expectations for the design of the DreamOn system, 21 respondents were recruited via convenience sampling to participate in a user perception survey. Due to this, most of the respondents are fellow college students.
	
	\subsection{Prototype Design}
	The main focus of this research is DreamOn, a prototype AI chatbot developed to investigate the role of affective feedback in the context of digital dream journaling. The development followed an iterative process. An initial version of the prototype was developed as a baseline, featuring the core chatbot functionality but without the adaptive color and music changes. This allowed for a focused evaluation of the aesthetic feedback in the current iteration. The system was designed to move beyond purely textual interaction and explore how non-verbal, aesthetic cues can shape the user's experience, specifically colors and music.
	
	\subsubsection{System Architecture and Interaction Flow}
	The DreamOn system is modeled around a straightforward flow designed to respond to the user's perceived emotional state. The process is initiated when a user inputs a textual description of their dream into the chatbot's interface. This text is then passed to a back-end emotion detection module for analysis. The module processes the narrative and classifies the user's latent emotion into one of six predefined categories. Upon successful classification, the system presents feedback to the application's user interface, dynamically altering the background color palette and playing a music track thematically aligned with the detected emotion. It is important to note that the current prototype's adaptive response mechanism is intentionally focused on these two modalities—visual aesthetics and auditory cues—to isolate and study their specific impact on user perception.
	
	\subsubsection{Emotion Detection and Classification}
	The core of the DreamOn system's intelligence lies in its text-oriented emotion recognition module. The initial model was developed using the Google GoEmotions dataset, a large, human-tagged collection of Reddit comments labeled into 27 emotion categories. The model was then fine-tuned to classify textual input into one of six basic emotions, as originally theorized by Ekman: Angry, Disgust, Fear, Joy, Sadness, and Surprise. To ensure applicability to a diverse user base, the model was created and evaluated using dreams written in both English and Tagalog, mirroring the bilingual environment of the targeted audience. This functionality enables the system to handle a broader variety of natural user inputs without limitations of language.
	
	\subsubsection{Design of Adaptive Aesthetic Feedback}
	The main objective of this research is to explore the potential of non-verbal cues to convey empathy, an area often less frequently considered than dialogue in HCI research. The design of DreamOn's adaptive feedback was based on concepts of color psychology and emotional connections. Each one of the six detectable emotions is mapped to a distinct interface theme, comprising a specific color palette and a matching musical genre. The mapping used is meant to validate, soothe, or otherwise appropriately respond to the user's emotional state. Table~\ref{tab:theme_mappings} provides a comprehensive explanation for these mappings of emotions to interfaces.
	
	The choice of the music tracks was influenced by the same psychological principles. For example, the "Uplifting Theme" evoked by anger is paired with cheerful and hopeful music, whereas the "Emotional Theme" for sadness includes a more melancholic and soft soundscape. This combined method seeks to establish a comprehensive and engaging emotional atmosphere for the user.
	
	
	\subsection{Data Collection}
	The evaluation of the DreamOn concept was designed to address the research from two angles: how reliable it is in detecting the user’s emotion and the user’s perception of if they felt the application was empathetic towards them.
	
	\subsubsection{Research Question and Design}
	This study was guided by this research question: To what extent do adaptive auditory and visual feedback mechanisms, driven by automated emotion detection, influence the perception of empathy in a digital dream journaling chatbot? To answer this question, we approached it with these two stages:
	\begin{itemize}
		\item \textbf{Technical Validation:} A quantitative assessment of the emotion detection model's accuracy in classification utilizing a regulated dataset.
		\item \textbf{User Perception Survey:} A survey incorporating both quantitative (Likert scale, multiple-choice) and qualitative (open-ended) questions to evaluate user attitudes, preferences, and expectations regarding empathetic technology, prior to any interaction with the DreamOn prototype. This provides a crucial baseline of user needs.
	\end{itemize}
	
	\subsubsection{Protocol for Technical Validation}
	The technical performance of the model was evaluated to establish its reliability. A tailored dataset of 30 short dreams was created specifically for this validation. The dataset was evenly distributed, featuring precisely five unique scenarios for each of the six target emotions (Anger, Disgust, Fear, Joy, Sadness, Surprise). To also cater to Filipino users, the narratives were composed in both English and Tagalog (Three English and two Tagalog narratives per emotion).
	The validation process involved feeding each of the 30 narratives into the DreamOn chatbot. The model's predicted emotion for each narrative was systematically recorded. The output was then compared against the actual emotion. This was done to get the model’s classification accuracy, which was calculated for each of the six emotions individually and as an overall system average.
	
	\subsubsection{Protocol for User Perception Survey}
	An online survey was conducted to gather data on user perspectives before introducing them to the DreamOn prototype. The survey was organized into multiple sections:
	\paragraph{Part 1: Experience and Importance} This section featured inquiries regarding the respondents’ previous engagement with digital journaling or mood-tracking applications. It also employed a 5-point Likert scale to assess how important users felt an app was in providing emotional support when talking about personal subjects such as dreams.
	\paragraph{Part 2: Feature Preferences} This section asked participants to select or rank their preferred feedback styles (e.g., "Calm and validating," "Friendly and casual") along with the UI components they thought would best represent their emotional state (e.g., "Background hue," "Mood music," "Tone/style of system notifications"). Questions also directly evaluated the perceived influence of visuals and the attractiveness of adaptive features.
	\paragraph{Part 3: Qualitative Probes} This section featured open-ended questions designed to gather deeper, more intricate information. Participants were requested to explain what contributes to a digital interaction feeling "robotic or dismissive" and to provide details on their positive or negative experiences with current self-reflection applications.
	Quantitative data from the survey were analyzed through descriptive statistics to see common characteristics. Qualitative data from the open-ended questions underwent thematic analysis, in which responses were coded and classified to reveal common patterns, themes, and important user insights.
	
	\section{Results}
	This section showcases the results from the evaluation protocol consisting of two parts: the technical performance of the DreamOn system's emotion detection model, followed by the quantitative and qualitative results from the user perception survey.
	
	\subsection{Technical Validation: Emotion Detection Performance}
	The evaluation of the emotion detection model yielded a distinct understanding of its strengths and weaknesses, delivering essential context for its possible use in a user-facing application.
	
	\subsubsection{Overall System Accuracy}
	The emotion detection model attained an overall classification accuracy of 86.67\% across the entire set of 30 dream narratives. This figure is derived from 26 accurate predictions made out of a total of 30 scenarios. This outcome suggests that the model is typically effective and operates consistently in most scenarios, offering a robust basis for a flexible system.
	
	\subsubsection{Per-Emotion Accuracy and Error Analysis}
	Although the overall accuracy is high, a more detailed examination shows notable differences in performance among the six emotion categories. The model's per-emotion accuracy and specific error patterns are detailed in the confusion matrix in Table~\ref{tab:confusion_matrix}.
	
	\begin{table}[t]
		\caption{CONFUSION MATRIX AND PER-EMOTION ACCURACY}
		\label{tab:confusion_matrix}
		\centering
		\resizebox{\columnwidth}{!}{%
			\begin{tabular}{|l|cccccc|c|}
				\hline
				\textbf{True} & \multicolumn{6}{c|}{\textbf{Predicted}} & \textbf{} \\
				\textbf{Emotion} & \textbf{Angry} & \textbf{Disgust} & \textbf{Fear} & \textbf{Joy} & \textbf{Sadness} & \textbf{Surprise} & \textbf{Accuracy} \\ \hline
				Angry & 5 & 0 & 0 & 0 & 0 & 0 & 100\% \\
				Disgust & 0 & 3 & 1 & 0 & 1 & 0 & 60\% \\
				Fear & 0 & 0 & 4 & 0 & 1 & 0 & 80\% \\
				Joy & 0 & 0 & 0 & 5 & 0 & 0 & 100\% \\
				Sadness & 0 & 0 & 0 & 0 & 5 & 0 & 100\% \\
				Surprise & 0 & 0 & 0 & 1 & 0 & 4 & 80\% \\ \hline
			\end{tabular}
		}
	\end{table}
	
	The analysis highlights several key findings:
	\begin{itemize}
		\item \textbf{High Performance:} The model demonstrated perfect accuracy (100\%) for the emotions of Joy (5/5 correct), Sadness (5/5 correct), and Angry (5/5 correct). This suggests a high degree of reliability in identifying these basic emotional states.
		\item \textbf{Moderate Performance:} The model performed well, but not perfectly, for Fear (4/5, 80\% accuracy) and Surprise (4/5, 80\% accuracy).
		\item \textbf{Poor Performance:} The model struggled most with Disgust, achieving only 60\% accuracy (3/5 correct). This category represents the model's primary weakness.
	\end{itemize}
	
	An in-depth analysis of the misclassifications reveals important insights into the model's distinct error patterns:
	\begin{itemize}
		\item One scenario intended to evoke Disgust (``\textit{I dreamt that I was eating my favorite meal when I discovered a dead cockroach...}'') was misclassified as Fear.
		\item Another Disgust scenario, written in Tagalog (``\textit{Tumapak ako sa may kanal na puno ng basura...}''), was misclassified as Sadness.
		\item One Fear scenario (``\textit{May humila sa akin pababa habang naliligo sa ilog...}'') was misclassified as Sadness.
		\item One Surprise scenario (``\textit{I dreamt that I opened my front door and found a long-lost friend...}'') was misclassified as Joy.
	\end{itemize}
	These specific errors, particularly the confusion between semantically adjacent but experientially distinct emotions like Disgust/Sadness and Surprise/Joy, are critical for understanding the potential user experience implications.
	
	\subsection{User Perception Survey Findings}
	The survey of 21 potential users provided a rich dataset of attitudes and preferences, establishing a clear user-centered foundation for the design of empathetic digital tools.
	
	\subsubsection{Quantitative Analysis of User Preferences}
	The quantitative data from the survey reveal a strong user appetite for emotionally aware and adaptive features in applications for personal reflection. A summary of key responses is presented in Table~\ref{tab:survey_results}.
	
	\begin{table}[t]
		\caption{QUANTITATIVE USER PREFERENCE SURVEY RESULTS (N=21)}
		\label{tab:survey_results}
		\centering
		\begin{tabular}{|p{0.45\columnwidth}|p{0.45\columnwidth}|}
			\hline
			\textbf{Survey Question / Topic} & \textbf{Response Distribution (Count \& \%)} \\ \hline
			\textbf{How important is emotionally supportive feedback?} & 
			Very important (5): 3 (14.3\%) \newline 
			Important (4): 6 (28.6\%) \newline 
			Moderately important (3): 4 (19.0\%) \newline 
			Slightly important (2): 4 (19.0\%) \newline 
			Not important (1): 4 (19.0\%) \\ \hline
			\textbf{Do visuals affect how emotionally supported you feel?} & 
			Very much (5): 8 (38.1\%) \newline 
			Quite a bit (4): 6 (28.6\%) \newline 
			Somewhat (3): 4 (19.0\%) \newline 
			Slightly (2): 3 (14.3\%) \newline 
			Not at all (1): 0 (0\%) \\ \hline
			\textbf{Would you prefer if the app included calming music?} & 
			Yes: 16 (76.2\%) \newline 
			No: 5 (23.8\%) \\ \hline
			\textbf{How likely are you to use an app that adjusts its mood?} & 
			Very likely (5): 6 (28.6\%) \newline 
			Likely (4): 7 (33.3\%) \newline 
			Moderately likely (3): 5 (23.8\%) \newline 
			Slightly likely (2): 1 (4.8\%) \newline 
			Not likely at all (1): 2 (9.5\%) \\ \hline
			\textbf{Which feedback style feels most supportive? (Multiple selections allowed)} & 
			Calm and validating: 14 \newline 
			Friendly and casual: 11 \newline 
			Professional and neutral: 6 \newline 
			I don’t want feedback: 2 \\ \hline
		\end{tabular}
	\end{table}
	
	The trends seen from this data were:
	\begin{itemize}
		\item \textbf{Importance of Support:} A notable share of users (45.4\%) categorized emotional support as "Important" or "Very important." Although there was a range throughout the spectrum, the desire for support stands out as a significant inclination.
		\item \textbf{Impact of Visuals:} A significant majority of users (68.2\%) reported that visuals influence their feelings of emotional support either "Quite a bit" or "Very much." No participant believed that visuals had no impact whatsoever.
		\item \textbf{Preference for Music:} A vast majority (77.3\%) indicated that they would like a dream-sharing app to feature soothing music or sounds.
		\item \textbf{Desire for Adaptivity:} A majority of users are open to an adaptable interface, as 63.7\% report being "Likely" or "Very likely" to engage with an app that modifies its atmosphere according to their dream journaling.
		\item \textbf{Preferred Style:} When discussing feedback styles, "Calm and validating" was the most commonly chosen option, with "Friendly and casual" as a close second, showing a preference for supportive and affirming communication rather than neutral or formal approaches.
	\end{itemize}
	
	\subsubsection{Qualitative Themes of Digital Empathy and Dismissiveness}
	The thematic analysis of open-ended responses offered greater insight into the quantitative results, highlighting how users differentiate between empathetic and robotic elements in digital interactions.
	
	\paragraph{Theme 1: Hallmarks of a "Robotic" Interaction}
	When those who had previously considered an app's response to be dismissive were asked about the reasons for this sentiment, a distinct and consistent theme appeared, focused on a perceived absence of authenticity and personalization. The subtopics consist of:
	\begin{itemize}
		\item \textbf{Generic and Canned Responses:} This was the chief sub-theme. Users highlighted "clearly pre-prepared/canned replies," a "clear structure/pattern that AI typically uses," and communications you "would immediately recognize... were crafted to convey that." This implies that predictability and the absence of novelty are key indicators of a robotic interaction.
		\item \textbf{Tonal Incongruence:} A significant failure point highlighted by one user occurred when the "Tone is inconsistent with the present mood." This underscores the significance of not only grasping the content but also aligning with the emotional tone of the interaction. A response that doesn’t match the tone, even if sincere, can seem shocking and dismissive.
		\item \textbf{Perceived Lack of Understanding:} Responses were characterized as "official and devoid of understanding" and exhibited a "fairly neutral" vocal tone. In an emotional setting, this neutrality was viewed not as impartial but as a deficiency in involvement and compassion.
	\end{itemize}
	
	\paragraph{Theme 2: Desired Features for Supportive Reflection}
	When asked about their likes and dislikes regarding current journaling and mindfulness applications, participants' answers aligned with themes of interactivity, design, and effectiveness:
	\begin{itemize}
		\item \textbf{Dislike of Passivity:} A major source of dissatisfaction was the "absence of feedback" from applications that acted solely as inactive storage for ideas. Users indicated a wish for increased interaction beyond mere data input.
		\item \textbf{Value of Aesthetics:} The sensory experience was greatly appreciated. A user felt discouraged by the "limited design options or its simplicity," suggesting that visual appeal influences ongoing participation. In contrast, a different user commended a meditation application for the "narrator's voice being calm and soothing," emphasizing the benefits of quality audio feedback.
		\item \textbf{Tension in Simplicity:} A significant tension was observed between the wish for efficiency and the requirement for captivating design. One user appreciated a "minimalistic" design that allows for "quick journaling without excessive features," while another considered the "basic-ness" to be discouraging. This indicates that a delicate equilibrium needs to be achieved between an efficient user experience and one that is visually and emotionally appealing.
	\end{itemize}
	
	
	\section{Discussion}
	\subsection{\textbf{Technical Performance}}
    Overall, the accuracy of the emotion detection model was 86. 67\%, which is a promising result, but it also highlights critical nuances in the ability of the emotion detection model to accurately classify select emotions.
    \paragraph{High-Performing or High Reliability Emotions}
    The model showed perfect accuracy for the following emotions: \textit{Anger, Joy and Sadness}. A good result even given its small sample convenience size. This reliability could be due to how these emotions reflect the concept established by Ekman's that these are evolutionarily distinct feelings that have increased linguistic identification of them (i.e., feeling blue is an expression of sadness). This overall implies the model is dependable in the management of high-arousal states.
    \paragraph{Low-Performing or Low Reliability Emotions}
    The model highlights relatively low accuracies for \textit{Disgust} (60\%), \textit{Surprise} and \textit{Joy} (80\%), all of which indicate an extent of semantic ambiguity. For disgust, the misclassifications seem to commonly stem from overlapping experiential descriptors (i.e  Disgust (“I dreamt that
I was eating my favorite meal when I discovered a dead
cockroach...”) which was misclassified as fear.) This also indicates how disgust is in high need of context-dependency research or support and how culturally nuanced it may be, especially in Tagalog contexts.
Lastly, the results indicate a confusion between \textit{surprise} and \textit{joy}. A confusion that underscored how positively framed surprises can blur emotional boundaries just like this one. More research for future models needs better sentiment analysis to distinguish this type of valence. 
\newline 
Overall, the model seems to be viable for the core emotions, but there is a great indicator of the need for enhanced training with disgust and its culture-specific narratives. There is also a need for a more in-depth sentiment analysis between the emotions of \textit{Surprise} and \textit{Joy}.

\subsection{\textbf{User Perceptions Insight}s}
\paragraph{\textbf{Affinity for Visual and Auditory Elements}}
The impact of visuals on perceived empathy was highlighted by (68.2\%) percent of the users whereas music is appreciated by (77.3\%) percent of the users, which confirms the choice of DreamOn to focus on aesthetic feedback. The tendency towards selecting of such calming, affirming voice tones (Table III) goes in line with the themes of Table I (e.g., blue to evoke the feeling of sadness as inducing sympathy).

\paragraph{\textbf{Positive perception to adaptive emotion}}
Majority (63.7\%) preferred dynamic personalization (e.g., changing colors/music) as interfaces that adapt on their moods,validating that changing colors/music is an indicator of responsiveness, a key marker of empathy in HCI.  

\paragraph{\textbf{Perception to Robotic Interactions}}

 Users interpreted canned or "information heavy" responses as being dismissive. The daily dream journal  an extremely individual experience, which means the answers should be answered in the manner that feels personal, which is an understandable response. This is an indicator for greater emotional responsiveness and empathy.

\paragraph{\textbf{Tonal Incongruence}} Since system tone was cheerful music, incongruence in tonal emotion of the user (sadness) compounded irritation due to perceived insensitiveness. This suggest an improvement on the emphasis on emotion-to-feedback harmony.

\paragraph{\textbf{Friction Based on Passiveness}} People did not like how the application effectively turned into passive storage since they wanted empathy to engage with them in a proactive manner. (e.g., through responsive feedback).\newline

\textbf{Final Discussion Thoughts:}\newline
The DreamOn study shows that empathy can be effectively achieved in chat robots through multimodal feedback, localized to both accuracies (86.67\% in emotion detection) and responsive visual and auditory reactions, and 63.7\% of the users reported comfy to such an arrangement. The system was good at identifying the core emotions such as anger, joy and sadness, but it had a weaker score when identifying the more complex emotions such as disgust (60\%) or that it would sometimes conflate surprise and joy, demonstrating significant semantic and cultural deficiencies in emotion recognition. Users stressed that empathy requires personal and tonally consistent communication, and they did not find alike or inappropriate reactions to be caring. Nonetheless, the shortcomings like sampling biases (mostly college students), feedback variety (only color and music), and the absence of long-term engagement statistics are some of the opportunities to enhance. We hope from this, the model was able to at least show the potential of the integration of these multi-modal systems with user-perceived empathy and the need to account for "AI empathy" or perception to wider range of responses (i.e, cultural diversity, technical resilience).




\section{Conclusion and Future Work}
DreamOn shows that individualization and thoughtfully constructed visual metaphors are instrumental when it comes to the empathic AI interaction in the dream journaling app. The system was capable of adapting color schemes and music to their moods, making it seem to produce good response in terms of the users feeling they were being understood. Nonetheless, the problem with the existing implementation is the limited taxonomy of dreams that might not fully represent the entire "range" of the dream experiences and feelings relative to that particular dream experience.

Future development: In the current implementation there is a lack of modalities to capture subtle cues of the emotional state, to resolve that this may be extended using voice patterns or analysis of emotion detection via EEG. There is also a necessity to conduct longitudinal studies to analyze how the continued exposure to the empathic chatbots can affect user engagement and emotional well-being in the long-term perspective. Work on these topics would help bring forth the creation of more mature and truly supportive forms of AI companions to reflect on oneself and help in maintaining mental health.
	
	
	\begin{thebibliography}{00}
		\bibitem{b1} A. K. Fetterman, M. D. Robinson, and B. P. Meier, ``Anger as `seeing red': evidence for a perceptual association,'' \textit{Cognit. Emot.}, vol. 26, no. 8, pp. 1480--1489, 2012.
		\bibitem{b2} L. B. Wexner, ``The degree to which colors (hues) are associated with mood-tones,'' \textit{J. Appl. Psychol.}, vol. 38, no. 6, p. 432, 1954.
		\bibitem{b3} M. Berthold, and R. Ammann, ``How Colours Affect Us,'' in \textit{Knowledge and Space}, vol. 17, Springer, Cham, 2022, pp. 247-253.
		\bibitem{b4} S. Schnall, J. Benton, and S. Harvey, ``With a clean conscience: Cleanliness reduces the severity of moral judgments,'' \textit{Psychol. Sci.}, vol. 19, no. 12, pp. 1219--1222, 2008.
		\bibitem{b5} R. M. Gerard, ``Differential effects of colored lights on psychophysiological functions,'' Ph.D. dissertation, Univ. of California, Los Angeles, CA, USA, 1958.
		\bibitem{b6} N. Kaya, and H. H. Epps, ``Relationship between color and emotion: A study of college students,'' in \textit{Proc. Centennial Conf. Center Human-Environ.}, 2004, pp. 1--7.
		\bibitem{b7} C. A. Thorstenson, A. D. Pazda, and A. J. Elliot, ``Sadness and synesthesia: The impact of emotion on color-grapheme pairings,'' \textit{Conscious. Cogn.}, vol. 33, pp. 384--391, 2015.
        \bibitem{b8} S. E. Palmer, K. B. Schloss, Z. Xu, and L. R. Prado-León, ``Music–color associations are mediated by emotion,'' \textit{Proc. Natl. Acad. Sci. U.S.A.}, vol. 110, no. 22, pp. 8836--8841, 2013.
        \bibitem{b9} A. P. Whiteford, D. Krefting, M. W. Rees-Jones, and S. E. Palmer, ``Color–music correspondences are mediated by emotion,'' \textit{Psychol. Sci.}, vol. 29, no. 7, pp. 1031--1039, 2018.
        \bibitem{b10} R. J. Safran and N. Sanda, ``Color synesthesia and emotion: An overview,'' \textit{Conscious. Cogn.}, vol. 34, pp. 1--7, 2015.
        \bibitem{b11} W. Tschacher and G. Widmer, ``Empathy in music perception and the role of mirror neurons,'' \textit{Psychomusicology: Music, Mind, and Brain}, vol. 28, no. 2, pp. 123--134, 2018.
        \bibitem{b12} M. Blagrove, J. Hale, H. Lockheart, M. Carr, J. Jones, and J. Valli, ``Testing the Empathy Theory of Dreaming: The relationships between trait empathy, dream sharing, and trait alexithymia,'' \textit{Front. Psychol.}, vol. 10, article 1351, 2019.
        \bibitem{b13} M. Youvan, ``AI-trained dream interpretation systems for emotional resonance,'' \textit{Dream Cognit. Syst. J.}, vol. 12, no. 1, pp. 12--21, 2024.
        \bibitem{b14} L. Dorado, M. Gutiérrez, and P. Morales, ``Machine empathy via multisensory feedback: Effects of music and color lighting on stress,'' \textit{J. Ambient Intell. Humaniz. Comput.}, vol. 13, pp. 451--461, 2022.
        \bibitem{b15} X. Fei, H. Tanaka, and S. Murakami, ``EmpathyEar: A multimodal empathic chatbot using language, audio, and color cues,'' in \textit{Proc. Int. Conf. on Affective Comput. and Intell. Interaction (ACII)}, 2024, pp. 46--55.
        \bibitem{b16} P. J. Loewenstein, ``The influence of music on dream affect and narrative,'' \textit{J. Sleep Res.}, vol. 32, no. 1, e13755, 2023.
        \bibitem{b17} A. Z. D. Williams and R. J. Smith, ``Synesthetic processing of music and color: Implications for AI-mediated empathy,'' \textit{Neuropsychologia}, vol. 136, pp. 107246, 2020.
	\end{thebibliography}
\end{document}
